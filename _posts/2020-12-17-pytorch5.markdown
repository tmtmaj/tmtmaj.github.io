---
title: "﻿﻿﻿[Pytorch] 임베딩을 합치는 4가지 방법 4 Methods to combine embeddings(Representations)﻿"
layout: post
author: Jeonghyeok Park
categories: Pytorch
tags: Pytorch
---

﻿

안녕하세요.

### 4 Methods to combine embeddings(Representations)

임베딩을 합치기 위한 4가지 방법입니다. 

저에게 익숙한 자연어 처리의 관점으로 이야기해보겠습니다. 결국에는 matrix를 가지고 병합하는 과정이기에 batch 등 data의 특성을 고려해서 살짝씩 변경해주시면 어떤 task에서든  적용 가능합니다.

------

먼저 필요성에 대해서 짧게 말하겠습니다. 모델의 성능을 향상시키기 위해서 하는 방법은 대략 3가지 정도있겠네요. (paper를 쓸 건덕지가 되는 것들이죠)

**모델 확장**: 여러 모델을 합치거나 (e.g., CNN + RNN + CRF) 모델을 더 deep하게 만드는 것

**트레이닝 셋 이외의 외부 자원 사용**: 여기서 말하는 외부자원은 더 많은 트레이닝 셋을 말하는 것이 아닙니다. 트레이닝 셋에 관련하여 도움이 될 만한 정보를 말합니다. 예를 들어, 한국어 문장 (training set) "나는 너를 사랑해"에서 "나"는 주어이고 "너"는 목적어이고 "사랑해"는  동사이죠? 여기서 주어, 목적어, 동사가 외부 자원이 될 수 있습니다.

**하이퍼 파라미터 조정:** batch_size, learning rate 등등.. 참고로 BERT에서 RoBERTa로의 향상이 하이퍼 파라미터 조정이 얼마나 중요한지 알 수 있습니다. 

---

embedding을 합치는 것은 "**모델 확장**"이나 "**외부 자원 사용**"에서 사용될 수 있습니다. 예를 들어, RNN을 Bi-RNN으로 바꾸면 일반적으로 concatenation을 통해서 toward-left RNN과  toward-right 의 RNN hidden state를 잇는 방법으로 처리합니다.  (2 Matrix를  concatenation을 통해 이음)

또 다른 예로는 위에서 말씀드린 [주어, 목적어, 동사]를 embedding으로 만들어서 [나는, 너를, 사랑해]의  embedding에 주입해서 모델에게 '나는'이 주어이고 '너를'이 목적어라는 것을 학습하게 하고 싶다고 가정하겠습니다. 이  경우에 일반적으로 두개의 embedding table을 만듭니다. [나는, 너를, 사랑해] - word에 대한 임베딩 word  embedding이라고 합니다. 그리고 [주어, 목적어, 동사] - feature에 대한 임베딩 feature  embedding이라고 하겠습니다.  이 두 임베딩을 병합하기 위해 필요한 것이 오늘 소개해드릴 내용입니다. 참고로 바로 위  예시에 대해 더 궁금하시다면 *Rico Sennrich and Barry Haddow, Linguistic Input Features Improve Neural Machine Translation*를 한번 읽어보시길 추천드립니다.

---

### ﻿그럼 두 embedding을 병합하기 위한 방법에는 어떤 것들이 있을까요?

  

​     

#### ﻿**1. Concatenation : 두 임베딩을 잇음** 

[Code]

```
A_embedding= torch.randn([4, 5, 3]) # batch_size, sequence length, embedding_dim
B_embedding = torch.randn([4, 5, 3]) # batch_size, sequence length, embedding_dim

# Concatenation
torch.cat([A_embedding, B_embedding], -1).shape
```

[Output]

```python
torch.Size([4, 5, 6])
```

Concatenation을 수행하면 embedding_dim이 변하기 때문에 주의하셔야합니다.

original_embedding의 dim이 512였는데 additional_embedding이 256이라면 Concatenatino을 통해  concatenated_embedding의 dim은 768 (512 + 256)이 되기 때문에 모델의 dim을 768로 바꿔주거나 original_embedding의 dim을 256으로 줄여야합니다. 

실제 적용 시에는 사실 vocabulary의 크기에 따라 dim을 대략적으로 정합니다. word의 vocab 크기가 32000이고  feature (주어, 동사..)의 vocab의 크기가 20인데 word_embedding의 dim과  feature_embedding의 dim을 256으로 똑같이 맞추는 것은 성능을 오히려 떨어뜨릴겁니다. 이 경우 모델의 dim이  512라면 word_embedding의 dim을 500으로 feature_embedding의 dim을 12로 설정하는게 좋겠죠.



   

​       

#### **2. Addition: 두 임베딩을 더함**

[Code]

```
A_embedding= torch.randn([4, 5, 3]) # batch_size, sequence length, embedding_dim
B_embedding = torch.randn([4, 5, 3]) # batch_size, sequence length, embedding_dim

# ﻿Addition
torch.add(A_embedding, B_embedding).shape
```

[Output]

```python
torch.Size([4, 5, 3])
```

간단하지만 제 경험상으로는 꽤 괜찮은 방법입니다.



​    

#### **3. Averaging: 두 임베딩을 평균 냄**

[Code]

```
A_embedding= torch.randn([4, 5, 3]) # batch_size, sequence length, embedding_dim
B_embedding = torch.randn([4, 5, 3]) # batch_size, sequence length, embedding_dim

# ﻿Averaging
(torch.add(A_embedding, B_embedding)/2).shape
```

[Output]

```python
torch.Size([4, 5, 3])
```

﻿*Joshua Coates and Danushka Bollegala, Frustratingly Easy Meta-Embedding –  Computing Meta-Embeddings by Averaging Source Word Embeddings* 이 논문에서 concatenation과 averaging이 수학적으로 비슷한 정보를 유지할 수 있다는 것을 증명합니다. 하지만 제 경험상  다소 성능 차이가 있습니다. 



​    

#### **4. Using gate network: gate network을 통해 두 임베딩 병합**

[Code]

```
gate_layer = torch.nn.Linear(6, 3)
gate_sigmoid = torch.nn.Sigmoid()

def gate(A_embedding, B_embedding):
    AB_concat = torch.cat((A_embedding,B_embedding),-1)
    context_gate = gate_sigmoid(gate_layer(AB_concat))
    return torch.add(context_gate * A_embedding, (1.- context_gate) * B_embedding)  

A_embedding= torch.randn([4, 5, 3]) # batch_size, sequence length, embedding_dim
B_embedding = torch.randn([4, 5, 3]) # batch_size, sequence length, embedding_dim   

# Using gate network
gate(A_embedding, B_embedding).shape
```

[Output]

```python
torch.Size([4, 5, 3])
```

GRU에서 사용한 방법입니다. 참고할 논문으로는 <Jiacheng Yang et al., Towards Making the Most  of BERT in Neural Machine Translation>가 있습니다. 논문에서는 a dynamic  switching gate라는 이름으로 멋드러지게 설명하네요. 처음에는 experiment results 보고 '와 도대체 뭘  쓴거지.. 부럽다' 했는데 gate network이었습니다. 

------

### 고찰

어떤 방법이 더 좋다고 제가 단정지어서 말씀드릴 수는 없네요. 실제로 다양한 외부 자원을 NMT 주입하는 방법에 적용해본 결과  Concatenation, Gate network, Addition, Averaging 순으로 성능이 좋은 것 같습니다. 

혹은 외부 자원의 정보량에 따라 어느정도 선택의 방향은 잡을 수 있을 것 같습니다. 새로운 embedding이 원래 embedding과 비슷한 급의 정보를 가지고 있다면, Gate network, Addition, Averaging가 괜찮습니다. 하지만 새로운 embedding이 원래 embedding보다는 가지고 있는 정보량이 작다면 Concatenation을 적용하시는게  성능을 보존 혹은 약간의 향상 하는데 도움이 될 것입니다. 물론 새로운 embedding의 dim을 작게 조정해야겠죠. 

제 논문을 참고하셔도 될 것 같습니다. 

*Jeonghyeok Park and Hai Zhao., Korean Neural Machine Translation Using Hierarchical Word Structure*

한국어 자모와 음절 정보를 word embedding에 병합하는 방법으로 NMT task에서 BLEU 0.6 정도의 향상을 얻었습니다.  병합 방법으로는 Concatenation, Averaging, Using gate network을 적용했습니다.



﻿이번 포스팅은 여기까지입니다. 

감사합니다.

------

참고 자료:

https://github.com/keras-team/keras/issues/9485