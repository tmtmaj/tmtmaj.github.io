---
title: "[NLP] ﻿﻿Leveraging Pre-trained Checkpoints for Sequence Generation Tasks (TACL 2020) 정리"
layout: post
author: Jeonghyeok Park
categories: Natural-Language-Processing
tags: Natural-Language-Processing
---

안녕하세요.

이번 포스팅에서 소개해드릴 내용은 TACL 2020의 Leveraging Pre-trained Checkpoints for Sequence Generation Tasks을 정리한 내용입니다.
최근에 text summarization에 대한 논문을 훝어보다가 발견한 논문입니다.

한 줄로 정리하자면  

**Sequence Generation Tasks에서 공개된 Pre-trained language model의 checkpoint를 활용해서(parameter initializing) SOTA를 달성.**

논문 소개 전.. 얕은 저의 고찰입니다.
비슷한 방법을 이전부터 생각해왔고, 시도해봤지만 실패했던 내용에 대해 '사실은 그렇지 않다'라는 식의 논문이라 읽으면서 놀랍고 부러웠습니다.
2년전 BERT의 등장 이후, 비슷한류의 LM 모델(RoBERTa, XLnet, GPT)은 주로 NLU(Natural Lauguage Understanding)에서 매우 효과적이고 이제는 필수적인 방법으로 자리매김했습니다.
하지만 decoding text가 필요한 task에서는 그다지 효과적이지 않은 것 같았습니다.
예를 들어, machine translation, text summarization 등

그 이유에 대해서 짐작해보면, NLU task들은 BERT 위에 linear layer 하나만 더 쌓으면 바로 적용이 가능합니다.
모델의 짜임새가 비슷하죠. checkpoint를 load하고 linear layer > softmax 하면 거의 SOTA급의 성능이 나옵니다. 며칠 전에 NER 관련해서 BERT checkpoint를 사용해봤는데, 뒤에 linear layer도 쌓을 필요없더군요. BERTforclassfication를 받아서 모델의 output를 loss로 사용하면 끝이었습니다. 코드의 전처리 부분이 90%이고 model에 관련된 코드는 10줄이 안됐습니다.

하지만 machine tranlsation의 경우(transformer), encoder 부분은 같지만 decoder부분을 처리하기 애매하죠.
그래서 적용한 방법이

1. 공개된 BERT의 tokenizer를 이용하여 데이터를 tokenize하고 input > BERT > Transformer 의 방식으로 Transformer에 적용하는 방법 (마치 embedding 처럼)

2. Transformer의 multiattention 부분을 수정하여 BERT의 출력을 대입

다음은 Incorporating BERT into Neural Machine Translation에서 발췌한 그림입니다.
(한국어 기계번역에서 같은 방법을 시도했지만 오히려 성능이 더 떨어졌습니다.
BERT를 학습할 때 데이터가 부족했을 수도 있고, 한국어에 적합하지 않은 수도 있습니다)

--- 


![]({{ site.baseurl }}/assets/sgt1.png )

---


위의 방법에서 BERT는 당연히 Freezing됩니다.
저도 비슷한 방법을 몇 번 시도해보다 말았습니다.
당시에 BERT를 machine translation에 적용한 논문을 찾기 어려웠고, 아.. 세계의 고수들이 아직까지 그런 논문을 안내는 거보면 BERT는 machine translation에 적용하면 안되는거구나라고 일찍이 선 긋고 관둔 쪽에 더 가까운 것 같습니다.
그래서 그런지 이 논문을 보면서 감동을 주는 게 이런 거구나..라고 느꼈던 것 같습니다.
모델을 변형해야하는 것도 아니고, 간단한 방법(model parameter initializing)으로 PrLM을 sequence generating task에, 방대한 실험을 통해 PrLM이 효과가 있다는 것을 입증했습니다.

#### 간단한 방법

기존의 모델에 추가적인 모듈을 통해 성능을 향상시킨 방법들은 사실 재현하기가 어렵습니다. (아직 제 수준이 그 정도까지 가지 못했을 수도 있습니다..)
그런 논문(추가된 모듈, 다른 전처리 과정, 외부적인 소스 등을 통해 성능 향상)을 재현하다 보면 시간과 노력이 많이 필요하구나..라고 느낍니다.
먼저, 논문에 자세한 설명이 적혀있지 않아 저자에게 메일을 보내도 답장이 없습니다.
(답장을 받아도 제대로 설명을 해주는 저자는 몇 없습니다. 예의를 갖추지 않고 메일을 보낸 건 절대 아닙니다.)
모델 설명에서 큼지막한 부분만 설명이 되어있습니다. 예를 들어, additional multi attention model for specific objective를 encoder에 추가. 하지만 normalization 위치만 달라져도 모델 성능이 차이가 납니다.
이 밖에도 논문 재현에는 사소한 것 같기도 하고 중요한 것 같은 많은 애로 사항이 있지만, 이 정도만 적겠습니다.

#### 방대한 실험

일단, 4가지 task에 대해 실험했습니다. 1가지 task의 다른 4가지 dataset을 사용한 것이 아니라 4가지 다른 task입니다. 매우 믿음직스럽습니다.
또 BERT 뿐만 아니라 RoBERTa/ GPT-2도 사용했습니다.
아래에 실험 결과에서 표를 보여드릴 건데, model parameter를 초기화하는 것을 Random/BERT/GPT-2/RoBERTa를 사용합니다. Encoder와 Decoder를 나누어서 각자 초기화하기 때문에 조합을 생각해보면 12가지 (i.e. Ran2BERT, RoBERTa2GPT-2 등) 여기서 parameter share까지 생각하면.. 전반적인 task에서 성능이 하락한 조합은 없습니다. 그래도 거의 10가지 조합에 대한 실험을 진행합니다.

그럼 여기까지가 제 얕은 고찰이었고 논문 소개하겠습니다.

### Abstact

공개된 PrLM의 checkpoint로 NLP 연구자들은 시간을 절약하며 고성능의 모델을 구축할 수 있습니다.
하지만 지금까지의 PrLM의 주된 적용 방향은 NLU였습니다. (i.e. GLUE, SQuaD dataset)
이 논문에서는 Sequence Generation에서 PrLM checkpoint의 효용성에 대해 증명합니다.
실험에서는 기존의 PrLM의 checkpoint와 호환될 수 있는(모델의 구조가 같아 파라미터 초기화가 가능한) Transformer를 사용합니다.
BERT/GPT-2/RoBERTa의 checkpoint를 사용하여 Machine Translation, Text Summarization, Sentence Splitting, Sentence Fusion task에서 SOTA의 성능을 달성했습니다.

### Model

앞서 말씀드렸던 것처럼 모델에서 사용된 방법은 아주 간단합니다.
모든 task의 모델은 encoder-decoder Transformer model입니다.
모델의 파라미터 초기화에 사용될 PrLM은 BERT, GPT-2, RoBERTa2입니다.
PrLM checkpoint로 다이렉트하게 Transformer 모델을 초기화할 수는 없습니다.
먼저 모델의 모듈명 변경/내부 모듈의 matrix의 transpose/activation function 변경 등이 필요합니다.

글로벌하게 적용되는 변경 사항은 다음과 같습니다.

- PrLM 모델의 크기에 따라 Transformer의 모델을 변경 (예시, 12 layers/hidden size 768/FNN size 3072/12 attention heads

- Encoder: BERT Transformer를 이용합니다. 일반적으로 사용하는 Transformer layer와 약간 다릅니다. (예시, activation function이 ReLU에서 GeLU로 변경)

- Decoder: GPT-2의 경우, Transformer의 decoder layer를 사용하지만 내부에 encoder-decoder multi attention module은 없습니다. 따라서 encoder-decoder multi attention module 추가해줍니다.

각 PrLM의 checkpoints에 대한 설명은 논문이나 checkpoints를 제공하는 사이트에서 확인해주세요.
각 초기화 조합의 모델 파라미터에 대한 테이블입니다.

![]({{ site.baseurl }}/assets/sgt2.png )

실험에서는 총 10가지 조합에 대해서 실험을 진행합니다.
예를 들어, RND2GPT는 encoder는 Random으로 초기화된 것이고 decoder는 GPT로 초기화한 것입니다.
BERTSHARE는 BERT2BERT와 같지만 encoder와 decoder의 parameter를 공유합니다. 따라서 BERT2BERT보다 더 적은 파라미터로 학습이 가능합니다.
GPT는 encoder는 없고 오직 decoder만 있습니다. GPT의 checkpoints을 그대로 사용합니다.

실험을 진행할 4가지 task는 다음과 같습니다.

- Sentence fusion

the problem of combining multiple sentences into a single coherent sentence.
여러 문장을 자연스럽게 하나의 문장으로 합치는 task입니다. 저도 이번에 처음 알았네요.

- Split and Rephrase

The reverse task of sentence fusion is the split and-rephrase task, which requires rewriting a long sentence into two or more coherent short sentences.
sentence fusion과 반대의 task입니다. 한 문장을 여러 문장으로 나누는 task입니다.

- machine translation

공개된 BERT에 추가적으로 custom BERT에 대한 실험도 진행했습니다.

- Abstractive Summarization

### Experment

- sentence fusion

![]({{ site.baseurl }}/assets/sgt3.png )

evaluation metric은 SARI입니다.
모델의 출력을 여러개의 refernce와 비교하는 metric입니다.
SARI is a lexical similarity metric which compares the model’s output to multiple references and the input in order to assess the model’s ability to add, delete and keep an n-gram.
가장 위에 있는 Geva 2019는 vanilla transformer 모델을 사용한 결과입니다.

- Split and Rephrase

![]({{ site.baseurl }}/assets/sgt4.png )

- Machine translation

![]({{ site.baseurl }}/assets/sgt5.png )

오히려 떨어지는 경우도 있지만 custom BERT를 이용한 경우에 2~3 BLEU 정도 향상된 것을 확인할 수 있습니다. 놀랍고 부럽습니다.

- abstractive summarization

![]({{ site.baseurl }}/assets/sgt6.png )

#### 그리고 또 다시 고찰

연구 방향을 machine translation으로 선택하고 수십 편의 논문을 봐왔지만 이렇게 간단하지만 성능 향상 폭이 큰 논문은 거의 처음인 것 같습니다.
아직 재현을 해보지는 않았지만, (사실 github에서 제공하고 있지만 직접 재현해볼 계획입니다.) 한국어에도 적용이 되길 바랍니다.
가능하다면 BERT보다 학습 효율이 좋은(훈련 시간 단축) ELECTRA를 가지고 custom data를 학습시킨 후에 task에 적용하면 BERT보다 빠르고 더 좋은 결과들 얻을 수 있지 않을까 생각합니다.

이상입니다. 감사합니다. 
﻿